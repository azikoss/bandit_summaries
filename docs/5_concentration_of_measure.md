# 5. Concentration of Measure
<center>
<img width="480" src="./assets/5_normal_distribution.png">
</center>

Let's recall that in the Multi-Armed Bandit setting the pay-offs are initially unknown and we must learn them from data. This chapter introduces theory that explains **how to estimate the upper bound of mean rewards** by using tail probabilities. The introduced concepts are essential to design and understand many bandit algorithms. 
 
# Tail Probabilities
 **Tail probability** is a function of a random variable <img src="https://render.githubusercontent.com/render/math?math=X"> and a certain threshold <img src="https://render.githubusercontent.com/render/math?math=\epsilon > 0"> that **expresses the probability of <img src="https://render.githubusercontent.com/render/math?math=X"> being greater or smaller than <img src="https://render.githubusercontent.com/render/math?math=\epsilon">**.  
 
 We can apply this concept to express how much the sampling reward mean <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu}"> over/under-estimates the true reward mean <img src="https://render.githubusercontent.com/render/math?math=\mu">. This is formally called a **tail probability of <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu} - \mu">** and can be expressed as <img src="https://render.githubusercontent.com/render/math?math=\mathbb{P}(\hat{\mu} \geq \mu \%2B \epsilon)"> or <img src="https://render.githubusercontent.com/render/math?math=\mathbb{P}(\hat{\mu} \leq \mu - \epsilon)">. The first expression is called upper tail probability, it presents an upper bound of <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu}"> and is depicted in the picture above. The second is lower tail probability and it presents a lower bound of <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu}">. For instance, let <img src="https://render.githubusercontent.com/render/math?math=X ~ \N(\mu,\sigma)"> and <img src="https://render.githubusercontent.com/render/math?math=\epsilon = 2\sigma">, given the z-table, the upper tail probability of <img src="https://render.githubusercontent.com/render/math?math=\mathbb{P}(\hat{\mu} \geq \mu %2B 2\sigma)=2.3\%25">. 
 
One could think why not to use variance to express the spread of the sample mean <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu}">, defined as as <img src="https://render.githubusercontent.com/render/math?math=\mathbb{V}[\hat{\mu}] = \mathbb{E}[(\hat{\mu} - \mu)^2] =\dfrac{\sigma^2}{n}"> [[proof](https://youtu.be/7mYDHbrLEQo)]. The problem is that **variance only quantifies that the squared distance** between <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu}"> and <img src="https://render.githubusercontent.com/render/math?math=\mu"> and does **not tell much about the distribution of the error** between the sample and the true mean. If we would continue with the above example and say that we would pull an arm <img src="https://render.githubusercontent.com/render/math?math=n=100"> times, then <img src="https://render.githubusercontent.com/render/math?math=\mathbb{V}[\hat{\mu}] = \dfrac{\sigma^2}{100}">. This expression does not tell anything about the distribution of the distance between <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu}"> and <img src="https://render.githubusercontent.com/render/math?math=\mu">; we cannot say that the distance is with certain probability less than <img src="https://render.githubusercontent.com/render/math?math=\epsilon"> just like the tail probability function does.
 

## The Inequalities of Markov and Chebyshev
In the above example, we calculated the tail probability by using the z-table. In practice, we will **not have such comfort to work with normal distribution only**. Thus, we introduce two most common **inequalities that will help us to bound the tail of *any* random variable <img src="https://render.githubusercontent.com/render/math?math=X">**
1. Markov: <img src="https://render.githubusercontent.com/render/math?math=\mathbb{P}(X \geq \epsilon) \leq \dfrac{\mathbb{E}[X]}{\epsilon}">     
1. Chebyshev: <img src="https://render.githubusercontent.com/render/math?math=\mathbb{P}(|X - \mathbb{E}[X]| \geq \epsilon) \leq \dfrac{\mathbb{V}[X]}{\epsilon^2}">

Let's illustrate how both inequalities work in practice. We can make an example from any context, since <img src="https://render.githubusercontent.com/render/math?math=X"> can be *any* random variable. Let <img src="https://render.githubusercontent.com/render/math?math=X"> represent an outcome of a dice. The dice does not have to be fair, but for sake of this exercise let's say it is so <img src="https://render.githubusercontent.com/render/math?math=\mathbb{E}[X]=3.5">. Let's find out what is the probability that <img src="https://render.githubusercontent.com/render/math?math=X"> is at least <img src="https://render.githubusercontent.com/render/math?math=\epsilon=6">. Using the Markov inequality, <img src="https://render.githubusercontent.com/render/math?math=\mathbb{P}(X \geq 6) \leq \dfrac{3.5}{6} = 58.3\%25">. Thus, the probability of rolling 6 (and more) is at most 58.3%. As illustrated in this example, **Markov's inequality is quite loose** since the chance of rolling 6 on a fair dice is only 16.6%. To be exact, we bounded the probability of rolling *at least* six. Since the [proof](https://www.quora.com/What-is-an-intuitive-explanation-of-Markovs-inequality) of the Markov's inequality stems solely from the behavior of <img src="https://render.githubusercontent.com/render/math?math=X"> around its mean, it does not matter how many faces would the dice have and the estimate of 58.3% would still hold.  
 
To illustrate the Chebyshev inequality, we have to know the variance of <img src="https://render.githubusercontent.com/render/math?math=X">. The variance of a fair dice is 105/36~2.91. Let's bound the same event and express what is the upper tail probability of rolling at least 6. Given that the Chebyshev inequality bounds the distance in between <img src="https://render.githubusercontent.com/render/math?math=X"> and its expected value, we need to set <img src="https://render.githubusercontent.com/render/math?math=\epsilon=6-3.5=2.5"> to bound the probability of rolling (at least) 2.5 units from the expected value of 3.5; so essentially rolling 3.5+-2.5 which is equivalent to rolling 1 or 6. Using the Chebyshev inequality, <img src="https://render.githubusercontent.com/render/math?math=\mathbb{P}(|X - 3.5| \geq 2.5) \leq \dfrac{2.91}{2.5^2} = 46.6\%25">. Thus, the probability of rolling 1 or 6 is at most 46.6%. The bound is again quite loose as the actual probability is 16.6%+16.6% = 33.2%.

 
We can use **Chebyshev inequality and bound the sample reward mean <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu}">** (with variance <img src="https://render.githubusercontent.com/render/math?math=\mathbb{V}[\hat{\mu}] =\dfrac{\sigma^2}{n}">) as <img src="https://render.githubusercontent.com/render/math?math=\mathbb{P}(|\hat{\mu} - \mu| \geq \epsilon) \leq \dfrac{\sigma^2}{n\epsilon^2}">. This inequality is great because it can be used in any context. On the other hand, as we saw above, **the Chebyshev inequality can be quite loose** as it does not rely on any assumption. For that reason, we introduce subgaussian random variables and given their properties present tighter bounds.  


# Subgaussian Random Variables
 **A random variable <img src="https://render.githubusercontent.com/render/math?math=X"> is <img src="https://render.githubusercontent.com/render/math?math=\sigma">-subgaussian if it holds that <img src="https://render.githubusercontent.com/render/math?math=\mathbb{E}[e^{\lambda X}] \leq e^{\lambda^2 \sigma^2 / 2}"> for all <img src="https://render.githubusercontent.com/render/math?math=\lambda \in \mathbb{R}">**. This says that <img src="https://render.githubusercontent.com/render/math?math=X"> is a <img src="https://render.githubusercontent.com/render/math?math=\sigma">-subgaussian if the Laplace transform of <img src="https://render.githubusercontent.com/render/math?math=X"> is dominated by the Laplace transform of a Gaussian random variable with mean zero and variance <img src="https://render.githubusercontent.com/render/math?math=\sigma^2"> [[ref](http://www.stat.cmu.edu/~arinaldo/36788/subgaussians.pdf)]. The consequence of this is that subgaussian random variables are centered (<img src="https://render.githubusercontent.com/render/math?math=\mathbb{E}[X]=0">) and their variance is given by the subgaussian parameter <img src="https://render.githubusercontent.com/render/math?math=\sigma">. We say that the random variables that are not centered are <img src="https://render.githubusercontent.com/render/math?math=\sigma">-subgaussian if the noise <img src="https://render.githubusercontent.com/render/math?math=X-\mathbb{E}[X]"> is <img src="https://render.githubusercontent.com/render/math?math=\sigma">-subgaussian. Subgaussians have much lighter tails than Guassians (they decay at least as fast as the Gaussians) [[ref](https://statisfaction.wordpress.com/2017/05/02/sub-gaussian-property-for-the-beta-distribution-part-1/)] and they are more uniformly distributed around the mean than the Gaussians [[ref](https://www.researchgate.net/figure/Examples-of-Gaussian-supergaussian-and-subgaussian-distributions-All-distributions_fig3_228661138)]. 

A natural example of a subgaussian random variable is a centered Gaussian. If <img src="https://render.githubusercontent.com/render/math?math=N(0, \sigma^2)">, then <img src="https://render.githubusercontent.com/render/math?math=\mathbb{E}[e^{\lambda X}] = e^{\sigma^2 \lambda^2 / 2}"> [[proof](https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/MIT18_S997S15_Chapter1.pdf)] which is equal to the right-hand side of the definition of subgaussianity above. Thus, center Gaussian is a subgaussian random variable for all <img src="https://render.githubusercontent.com/render/math?math=\sigma">. 


Furthermore, 
 * if <img src="https://render.githubusercontent.com/render/math?math=X"> has zero mean and <img src="https://render.githubusercontent.com/render/math?math=|X| \leq B"> almost surely for <img src="https://render.githubusercontent.com/render/math?math=B \geq 0">, then <img src="https://render.githubusercontent.com/render/math?math=X"> is <img src="https://render.githubusercontent.com/render/math?math=B">-subgaussian 
 * if <img src="https://render.githubusercontent.com/render/math?math=X"> has zero mean and <img src="https://render.githubusercontent.com/render/math?math=X \in [a,b]"> almost surely, then <img src="https://render.githubusercontent.com/render/math?math=X"> is <img src="https://render.githubusercontent.com/render/math?math=(b-a)/2">-subgaussian (so the centered distribution of rolling dice <img src="https://render.githubusercontent.com/render/math?math=X-E[X]"> is 5/2-subgaussian) 
 
 
## Bounding Subgaussions
Next we present a **theorem that bounds subgaussian random variables**. If <img src="https://render.githubusercontent.com/render/math?math=x"> is <img src="https://render.githubusercontent.com/render/math?math=\sigma">-subgaussian, then for any <img src="https://render.githubusercontent.com/render/math?math=\epsilon \geq 0">, <img src="https://render.githubusercontent.com/render/math?math=\mathbb{P}(X \geq \epsilon) \leq \exp(-\frac{\epsilon ^ 2}{2\sigma^2})">. The proof  follows the Cremér-Chernoff method and goes as follows
1. <img src="https://render.githubusercontent.com/render/math?math=\mathbb{P}(X \geq \epsilon) = \mathbb{P}(\exp(\lambda X) \geq \exp(\lambda \epsilon))"> // the whole formula was exponentiated to the power of <img src="https://render.githubusercontent.com/render/math?math=\exp(\lambda)"> with constant <img src="https://render.githubusercontent.com/render/math?math=\lambda > 0"> to be tuned later
1. <img src="https://render.githubusercontent.com/render/math?math=\leq \mathbb{E}[\exp(\lambda X)] \exp(-\lambda \epsilon)"> // Markov's inequality was applied
1. <img src="https://render.githubusercontent.com/render/math?math=\leq \exp(\frac{\lambda^2 \sigma^2}{2}) \exp(-\lambda\epsilon)"> // the definition of subgaussianity was applied
1. <img src="https://render.githubusercontent.com/render/math?math=\= \exp(\frac{\lambda^2 \sigma^2}{2}-\lambda\epsilon)">
1. <img src="https://render.githubusercontent.com/render/math?math=\= \exp(-\frac{\epsilon^2}{2 \sigma^2})"> // by choosing <img src="https://render.githubusercontent.com/render/math?math=\lambda"> to be <img src="https://render.githubusercontent.com/render/math?math=\frac{\epsilon}{\sigma^2}">

By setting the right hand side of the equation <img src="https://render.githubusercontent.com/render/math?math=\exp(-\frac{\epsilon^2}{2\sigma^2})"> to <img src="https://render.githubusercontent.com/render/math?math=\delta">, solving this equation for <img src="https://render.githubusercontent.com/render/math?math=\epsilon"> and making the corresponding substitutions, we get an equivalent form of the above formula: <img src="https://render.githubusercontent.com/render/math?math=\mathbb{P}(X \geq \sqrt{2\sigma^2\log(1/\delta)}) \leq \delta">. This form is more convenient because it allows to easily bound the probability of the upper tail of <img src="https://render.githubusercontent.com/render/math?math=X"> by <img src="https://render.githubusercontent.com/render/math?math=\delta">. A similar inequality holds for the left tail.


Let's come back to our example and express the probability of rolling at least 6 on a dice. Since we have to center <img src="https://render.githubusercontent.com/render/math?math=X"> (by subtracting <img src="https://render.githubusercontent.com/render/math?math=E[X]">) for it to be subgaussian, the equivalent of rolling 6 on a centered distribution of  <img src="https://render.githubusercontent.com/render/math?math=X"> is 6-3.5=2.5. Given that the centered distribution of the dice rolls is 5/2-subgaussian, the probability of rolling (at least) 2.5 is <img src="https://render.githubusercontent.com/render/math?math=\mathbb{P}(X \geq 2.5) \leq \exp(-\frac{2.5 ^ 2}{2*(5/2)^2}) = 60.7\%25">. The obtained probability should be smaller than the one from Markov's inequality. I cannot explain why this did not happen. Please write to the [discussion](https://github.com/azikoss/bandit_summaries/discussions/categories/5-concentration-of-measure) section if you know.   
 
## Bounding the Sample Reward Mean
**To bound the tail behavior of <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu} - \mu">**, two more lemmas about the subgaussians random variables need to be introduced. Let <img src="https://render.githubusercontent.com/render/math?math=X"> be <img src="https://render.githubusercontent.com/render/math?math=\sigma">-subgaussian and <img src="https://render.githubusercontent.com/render/math?math=X_1"> and <img src="https://render.githubusercontent.com/render/math?math=X_2"> be independent and <img src="https://render.githubusercontent.com/render/math?math=\sigma_1"> and <img src="https://render.githubusercontent.com/render/math?math=\sigma_2">-subgaussian, respectively, then
1. <img src="https://render.githubusercontent.com/render/math?math=cX"> is <img src="https://render.githubusercontent.com/render/math?math=|c|\sigma">-subgaussian for all <img src="https://render.githubusercontent.com/render/math?math=c \in \mathbb{R}"> 
1. <img src="https://render.githubusercontent.com/render/math?math=X_1 \%2B X_2"> is <img src="https://render.githubusercontent.com/render/math?math=\sqrt{\sigma_1^2 \%2B \sigma_2^2}">-subgaussian 

 
**Presuming that <img src="https://render.githubusercontent.com/render/math?math=X_i - \mu"> are independent, <img src="https://render.githubusercontent.com/render/math?math=\sigma">-subgaussian random variable, then for any <img src="https://render.githubusercontent.com/render/math?math=\epsilon \geq 0">, <img src="https://render.githubusercontent.com/render/math?math=\mathbb{P}(\hat{\mu} - \mu \geq \epsilon) \leq \exp(-\frac{n\epsilon^2}{2\sigma^2})">, where <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu} = \frac{1}{n}\sum_{t=1}^{n} X_t">**. The proof goes as follows. By the lemmas in the above paragraph, it holds that <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu} - \mu = \sum_{t=1}^{n} (X_i - \mu)/n"> is <img src="https://render.githubusercontent.com/render/math?math=\sigma/\sqrt{n}">-subgaussian as each element in the sum is <img src="https://render.githubusercontent.com/render/math?math=\sigma/n">-subgaussian and the sum of <img src="https://render.githubusercontent.com/render/math?math=n"> such subgaussians is <img src="https://render.githubusercontent.com/render/math?math=\sqrt{n \frac{\sigma^2}{n^2}} = \sigma/\sqrt{n}">-subgaussian. Using the theorem on bounding the <img src="https://render.githubusercontent.com/render/math?math=\sigma/\sqrt{n}">-subgaussian random variable (while choosing <img src="https://render.githubusercontent.com/render/math?math=\lambda=\frac{\epsilon n}{\sigma^2}">) finalizes the proof. 
 

The above inequality presents a **stronger bound that the one obtained from Chebyshev's inequality** expect when <img src="https://render.githubusercontent.com/render/math?math=\epsilon"> is very small.

Similarly as for the general case, the inequality can be rewritten as <img src="https://render.githubusercontent.com/render/math?math=\mathbb{P}(\hat{\mu} \geq \mu \%2B \sqrt{\frac{2\sigma^2\log(1/\delta)}{n}}) \leq \delta">.


If you have any questions or comments, I would be happy if you write them in the [discussion](https://github.com/azikoss/bandit_summaries/discussions/categories/5-concentration-of-measure) section. 

# References
This text is *my* summary from the 5. Chapter of [Bandit Algorithm](https://tor-lattimore.com/downloads/book/book.pdf) book. The summary contains copy&pasted text from the book as well as some additional text. 
